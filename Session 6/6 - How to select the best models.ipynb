{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to select the best models\n",
    "\n",
    "## Predictions\n",
    "\n",
    "Before going into detail about model selection, we will first make predictions with multiple linear regression. Remember that if we want to make predictions, the go-to package in Python is *scikit-learn*. Since we often use the *statsmodels* package, I will show you how to make predictions using the *statsmodels* and the *scikit-learn* packages. We will use the box office example with advertising, number of screens, and rating (Rated-R) as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#Make the data\n",
    "#y are box office revenues, x1: advertising, x2: number of theaters, dummy: rated R  \n",
    "d = {\"y\":[23,12,36,27,45,70,55,8,62,28],\n",
    "     \"x1\":[29,49,89,110,210,190,153,20,122,41],\n",
    "     \"x2\": [2.036,2.919,1.707,1.505,2.232,2.910,2.795,1.46,3.288,1.838],\n",
    "     \"dummy\": [1,1,0,1,1,0,0,1,0,0]\n",
    "    }\n",
    "data = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.921\n",
      "Model:                            OLS   Adj. R-squared:                  0.881\n",
      "Method:                 Least Squares   F-statistic:                     23.31\n",
      "Date:                Tue, 13 Oct 2020   Prob (F-statistic):            0.00105\n",
      "Time:                        10:54:15   Log-Likelihood:                -31.386\n",
      "No. Observations:                  10   AIC:                             70.77\n",
      "Df Residuals:                       6   BIC:                             71.98\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         12.4045     10.382      1.195      0.277     -13.000      37.809\n",
      "x1             0.1880      0.040      4.723      0.003       0.091       0.285\n",
      "x2             6.1509      4.229      1.455      0.196      -4.197      16.499\n",
      "dummy        -17.6097      4.971     -3.542      0.012     -29.774      -5.445\n",
      "==============================================================================\n",
      "Omnibus:                        0.092   Durbin-Watson:                   2.439\n",
      "Prob(Omnibus):                  0.955   Jarque-Bera (JB):                0.191\n",
      "Skew:                           0.155   Prob(JB):                        0.909\n",
      "Kurtosis:                       2.400   Cond. No.                         599.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/scipy/stats/stats.py:1604: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=10\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "#Remember that the number of theaters was not signficant\n",
    "#Build OLS model\n",
    "data1 = sm.add_constant(data)\n",
    "lm_stats = sm.OLS(data1[\"y\"],data1[[\"const\",\"x1\", \"x2\", \"dummy\"]]).fit()\n",
    "print(lm_stats.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44.69562383])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's make a prediction to show that it does not matter whether a variable is signifcant or not\n",
    "#const:1, x1: 200, x2: 2, dummy: 1 \n",
    "lm_stats.predict([1,200,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44.696600000000004"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is exacty (small difference are due to rounding the coefficients to 4 digits)\n",
    "1*12.4045 + 200*0.1880 + 2*6.1509 + 1*(-17.6097)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE statsmodels: 17.233694068813882\n"
     ]
    }
   ],
   "source": [
    "#Now let's make a prediction with statsmodels\n",
    "np.random.seed(40)   \n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "#Make train/test spit\n",
    "train, test = train_test_split(data1, test_size=0.30)\n",
    "\n",
    "#Model and predict\n",
    "lm_stats = sm.OLS(train[\"y\"],train[[\"const\",\"x1\", \"x2\", \"dummy\"]]).fit()\n",
    "pred_stats = lm_stats.predict(test[[\"const\",\"x1\", \"x2\", \"dummy\"]])\n",
    "\n",
    "print(\"RMSE statsmodels: \"+str(np.sqrt(mse(test[[\"y\"]],pred_stats))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE sklearn: 17.23369406881387\n"
     ]
    }
   ],
   "source": [
    "#Now let's make a prediction with sklearn\n",
    "np.random.seed(40)   \n",
    "        \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.linear_model import LinearRegression  \n",
    "\n",
    "#Make train/test spit\n",
    "train, test = train_test_split(data1, test_size=0.30)\n",
    "\n",
    "#Model and predict\n",
    "lm_sk = LinearRegression().fit(train[[\"x1\", \"x2\", \"dummy\"]], train[\"y\"])\n",
    "pred_sk = lm_sk.predict(test[[\"x1\", \"x2\", \"dummy\"]])\n",
    "\n",
    "print(\"RMSE sklearn: \"+str(np.sqrt(mse(test[[\"y\"]],pred_sk))))\n",
    "#As expected the results are the same for sklearn as for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection with approximation measures\n",
    "\n",
    "We will now select the best model based on approximation measures: AIC, BIC, and adjusted R-squared. To select the best model, you must calculate all possible combinations of the predictors and see which one has the lowest AIC/BIC or the highest R-squared. Now, let's select the best two predictor models using AIC/BIC and adjusted R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>AIC</th>\n",
       "      <th>BIC</th>\n",
       "      <th>AdjR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lm_null.aic</td>\n",
       "      <td>64.410601</td>\n",
       "      <td>64.356511</td>\n",
       "      <td>-2.220446e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lm_x1.aic</td>\n",
       "      <td>49.039244</td>\n",
       "      <td>48.931064</td>\n",
       "      <td>8.996710e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lm_x2.aic</td>\n",
       "      <td>64.083676</td>\n",
       "      <td>63.975496</td>\n",
       "      <td>1.393749e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lm_dummy.aic</td>\n",
       "      <td>59.847519</td>\n",
       "      <td>59.739339</td>\n",
       "      <td>5.301123e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lm_x1x2.aic</td>\n",
       "      <td>49.552162</td>\n",
       "      <td>49.389892</td>\n",
       "      <td>8.985914e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>lm_x1dummy.aic</td>\n",
       "      <td>48.393964</td>\n",
       "      <td>48.231694</td>\n",
       "      <td>9.140556e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>lm_x2dummy.aic</td>\n",
       "      <td>57.667236</td>\n",
       "      <td>57.504966</td>\n",
       "      <td>6.767410e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model        AIC        BIC          AdjR\n",
       "0     lm_null.aic  64.410601  64.356511 -2.220446e-16\n",
       "1       lm_x1.aic  49.039244  48.931064  8.996710e-01\n",
       "2       lm_x2.aic  64.083676  63.975496  1.393749e-01\n",
       "3    lm_dummy.aic  59.847519  59.739339  5.301123e-01\n",
       "4     lm_x1x2.aic  49.552162  49.389892  8.985914e-01\n",
       "5  lm_x1dummy.aic  48.393964  48.231694  9.140556e-01\n",
       "6  lm_x2dummy.aic  57.667236  57.504966  6.767410e-01"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The AIC, BIC, and adjusted R-squared are built-in for in the statsmodels packake, so we will aslo focus on this one\n",
    "#We will only use the train set, since these measures will approximate the test set error\n",
    "\n",
    "#Always begin with a null model: model with only an intercept\n",
    "lm_null = sm.OLS(train[\"y\"],train[[\"const\"]]).fit()\n",
    "\n",
    "#One predictor models\n",
    "lm_x1 = sm.OLS(train[\"y\"],train[[\"const\",\"x1\"]]).fit()\n",
    "lm_x2 = sm.OLS(train[\"y\"],train[[\"const\",\"x2\"]]).fit()                              \n",
    "lm_dummy = sm.OLS(train[\"y\"],train[[\"const\",\"dummy\"]]).fit() \n",
    "\n",
    "#Two predictor models\n",
    "lm_x1x2 = sm.OLS(train[\"y\"],train[[\"const\",\"x1\", \"x2\"]]).fit()\n",
    "lm_x1dummy = sm.OLS(train[\"y\"],train[[\"const\",\"x1\", \"dummy\"]]).fit()                               \n",
    "lm_x2dummy = sm.OLS(train[\"y\"],train[[\"const\",\"x2\", \"dummy\"]]).fit() \n",
    "\n",
    "#Extract all measures\n",
    "results = pd.DataFrame(columns = [\"Model\",\"AIC\", \"BIC\", \"AdjR\"])\n",
    "results.Model = [\"lm_null.aic\", \"lm_x1.aic\", \"lm_x2.aic\", \"lm_dummy.aic\", \"lm_x1x2.aic\",\"lm_x1dummy.aic\", \n",
    "                            \"lm_x2dummy.aic\"]\n",
    "results.AIC = [lm_null.aic, lm_x1.aic, lm_x2.aic, lm_dummy.aic, lm_x1x2.aic,lm_x1dummy.aic, lm_x2dummy.aic]\n",
    "results.BIC = [lm_null.bic, lm_x1.bic, lm_x2.bic, lm_dummy.bic, lm_x1x2.bic,lm_x1dummy.bic, lm_x2dummy.bic]\n",
    "results.AdjR = [lm_null.rsquared_adj, lm_x1.rsquared_adj, lm_x2.rsquared_adj,\n",
    "        lm_dummy.rsquared_adj, lm_x1x2.rsquared_adj,lm_x1dummy.rsquared_adj, \n",
    "        lm_x2dummy.rsquared_adj]\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIC    48.393964\n",
       "BIC    48.231694\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's look at the minimimum value for AIC and BIC\n",
    "results[[\"AIC\", \"BIC\"]].apply(np.min)\n",
    "#This is the 5th model, so the model including advertising (x1) and rated-R (dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdjR    0.914056\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check whether the results are the same for the adj R\n",
    "results[[\"AdjR\"]].apply(np.max)\n",
    "#This is also the 5th model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection with cross-validation\n",
    "\n",
    "We know that the approximation measures only approximate the test set error. Now we will calculate the true test set error and see whether we get the same results. They are different scales, so might impact the different models in another way. So, in order to make the correct decision upon the best mode, it is best practice to first normalise the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before we will model cross-validation: we will make a pipeline to calculate the rmse\n",
    "#This will shorten the code and make it easier to model\n",
    "#We will use scikit-learn to fit the model here. Since we want to test a null model, we will not fit an intercept\n",
    "#Instead we will always add const as an intercept just as in statsmodels\n",
    "     \n",
    "def calculate_rmse(X, y, nFold):    \n",
    "    \n",
    "    from sklearn.pipeline import make_pipeline\n",
    "    #from sklearn.cross_validation import cross_val_predict\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "    from math import sqrt\n",
    "    \n",
    "    #Make sure that your estimator is from sklearn and not from statsmodels!\n",
    "    predictions = cross_val_predict(LinearRegression(fit_intercept=False), X, y, cv=nFold)\n",
    "    \n",
    "    rmse = sqrt(mse(y, predictions))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25.082195942301027,\n",
       " 14.63479475922388,\n",
       " 22.1417925478354,\n",
       " 18.239646987324686,\n",
       " 16.778593177750093,\n",
       " 8.472619796891246,\n",
       " 21.465823581001295,\n",
       " 21.465823581001295)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now calculate the RMSE of all the different models with 3 fold cross-validation\n",
    "#Note that we do not add the const here\n",
    "#Null model\n",
    "rmse_null = calculate_rmse(data1[[\"const\"]],data1[\"y\"],nFold=3)\n",
    "\n",
    "#One predictor models\n",
    "rmse_x1 = calculate_rmse(data1[[\"const\", \"x1\"]],data1[\"y\"],nFold=3)\n",
    "rmse_x2 = calculate_rmse(data1[[\"const\", \"x2\"]],data1[\"y\"],nFold=3)                              \n",
    "rmse_dummy = calculate_rmse(data1[[\"const\", \"dummy\"]],data1[\"y\"],nFold=3)\n",
    "\n",
    "#Two predictor models\n",
    "rmse_x1x2 = calculate_rmse(data1[[\"const\", \"x1\", \"x2\"]],data1[\"y\"],nFold=3)\n",
    "rmse_x1dummy = calculate_rmse(data1[[\"const\", \"x1\", \"dummy\"]],data1[\"y\"],nFold=3)                               \n",
    "rmse_x2dummy = calculate_rmse(data1[[\"const\", \"x2\",\"dummy\"]],data1[\"y\"],nFold=3)\n",
    "\n",
    "#Look at the best model\n",
    "rmse_null, rmse_x1, rmse_x2, rmse_dummy, rmse_x1x2, rmse_x1dummy, rmse_x2dummy, rmse_x2dummy\n",
    "#Again you can see"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, cross-validation and approximation measures give the same result. However, the results could have differed. In general, cross-validation is the best approach since you estimate the test set error directly. As you also might notice, this technique might become unmanageable when you have a lot of predictors. There are other model selection techniques (e.g. ridge, lasso, or elastic net regression) that are much more efficient. \n",
    "\n",
    "Also, if you have some proprietary knowledge (e.g. from talking with managers and experts in the field or by doing a literature review) about what variables to include in your model, you can narrow down the number of models to compare. However, one big downside of cross-validation is that when confronted with big dataset, it can take some time to get the performance measures. So in that case, approximation measures might be a good alternative. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
